{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Author: Ellie Dorland Advisor: Tyson Lee Swetnam About \u00b6 Ellie Dorland is a rising senior at City Highschool in Tucson, Arizona. During the summer of 2023 she is an intern at The University of Arizona through the BIO5 Institute's KEYS program. This summer she is an intern working at The University of Arizona in Dr. Tyson L. Swetnam's lab: a member of CyVerse , a cutting edge cyberinfrastructure funded by the National Science Foundation that is designed for research and committed to the principles of open science. This website follows the FAIR and CARE data principles and hopes to help further open science. Table of Contents \u00b6 Introduction Logbook Assignments KEYS 2023 Main Project My Project Covid Map Poster References Resources] Jupyter Notebooks Python Chat GPT GitHub KEYS 2023 Main Project \u00b6 The KEYS 2023 Main Project is the culmination of the program, where I applied the skills and knowledge I learned to a real-world problem. My project was about ... Results \u00b6 The results of my project are presented in a poster that summarizes my research question, methods, findings and conclusions. You can view the poster here. Chat GPT \u00b6 ChatGPT is an AI language model by OpenAI that generates human-like responses to text prompts, enabling interactive conversations and assisting with various tasks, from creative writting to code creation. For my project I used Chat GPT to help me write code and evaluate data faster.","title":"Introduction"},{"location":"#introduction","text":"Author: Ellie Dorland Advisor: Tyson Lee Swetnam","title":"Introduction"},{"location":"#about","text":"Ellie Dorland is a rising senior at City Highschool in Tucson, Arizona. During the summer of 2023 she is an intern at The University of Arizona through the BIO5 Institute's KEYS program. This summer she is an intern working at The University of Arizona in Dr. Tyson L. Swetnam's lab: a member of CyVerse , a cutting edge cyberinfrastructure funded by the National Science Foundation that is designed for research and committed to the principles of open science. This website follows the FAIR and CARE data principles and hopes to help further open science.","title":"About"},{"location":"#table-of-contents","text":"Introduction Logbook Assignments KEYS 2023 Main Project My Project Covid Map Poster References Resources] Jupyter Notebooks Python Chat GPT GitHub","title":"Table of Contents"},{"location":"#keys-2023-main-project","text":"The KEYS 2023 Main Project is the culmination of the program, where I applied the skills and knowledge I learned to a real-world problem. My project was about ...","title":"KEYS 2023 Main Project"},{"location":"#results","text":"The results of my project are presented in a poster that summarizes my research question, methods, findings and conclusions. You can view the poster here.","title":"Results"},{"location":"#chat-gpt","text":"ChatGPT is an AI language model by OpenAI that generates human-like responses to text prompts, enabling interactive conversations and assisting with various tasks, from creative writting to code creation. For my project I used Chat GPT to help me write code and evaluate data faster.","title":"Chat GPT"},{"location":"chatgpt/","text":"ChatGPT, a dynamic and ever-evolving AI system, is revolutionizing diverse fields, from creative scriptwriting to scientific investigations. As we harness its power, it is essential to master the art of extracting precise and reliable information from this remarkable tool. To achieve optimal results, we must adhere to three fundamental principles: assigning a role, providing detailed instructions, and specifying the desired output format. Assigning a Role: Setting the Context for Success \u00b6 Assigning a role to ChatGPT sets the stage for focused and accurate responses. Providing context helps ChatGPT align its responses with the intended purpose. Designating ChatGPT as a teacher, interpreter, or researcher clarifies its role and elicits responses aligned with scientific objectives. Detailed Instructions: Navigating the Path of Precision \u00b6 Precision is essential in extracting valuable insights from ChatGPT. Articulating scientific inquiries with specificity ensures clarity. Formulating detailed questions and tasks guides ChatGPT towards the desired outcome. Specifying Output Format: Tailoring Knowledge Delivery \u00b6 The choice of output format is crucial for effective scientific communication. Specifying how information is presented enhances comprehension and integration. Output formats can include paragraphs, code snippets, visualizations, or poetic expressions. By adhering to these three principles, we unlock the full potential of ChatGPT, ensuring accuracy and precision in our quest for knowledge and harnessing the limitless possibilities of artificial intelligence. Here are some examples showcasing how the interplay of role assignment, specific prompts, and defined output formats can yield fruitful outcomes: Examples: \u00b6 Prompt: you are a python interpreter and coder. please write me some code that would take state names, city names or county names and translate that into longitude and latitude qoordinates so that they can be used for computing Responce: To convert state, city, or county names into latitude and longitude coordinates, you can use a geocoding service or a geocoding library in Python. One popular geocoding library is geopy , which provides an interface to various geocoding services. Here's an example of how you can use geopy to retrieve the coordinates: from geopy.geocoders import Nominatim def get_coordinates ( location ): geolocator = Nominatim ( user_agent = \"my-application\" ) location_data = geolocator . geocode ( location ) if location_data is not None : latitude = location_data . latitude longitude = location_data . longitude return latitude , longitude else : return None # Example usage location = \"New York, NY\" coordinates = get_coordinates ( location ) if coordinates is not None : latitude , longitude = coordinates print ( f \"Coordinates for { location } : Latitude= { latitude } , Longitude= { longitude } \" ) else : print ( \"Coordinates not found for the location.\" ) In the code above, the get_coordinates() function takes a location name as input and uses the Nominatim geocoder from geopy to retrieve the coordinates. It returns a tuple of latitude and longitude if the coordinates are found, or None if the coordinates are not found. You can call the get_coordinates() function with state names, city names, or county names to retrieve their respective latitude and longitude coordinates. Remember to install the geopy library by running pip install geopy before executing the code. Clarification: can you make it so that it could draw this data from a spreadsheet that has many different names on it, but they are all in a collum called \"states\" Responce: Certainly! To read the data from a spreadsheet column named \"states\" and retrieve the latitude and longitude coordinates for each state name, you can use the pandas library to load the spreadsheet data and iterate over the \"states\" column. Here's an updated version of the code: import pandas as pd from geopy.geocoders import Nominatim def get_coordinates ( location ): geolocator = Nominatim ( user_agent = \"my-application\" ) location_data = geolocator . geocode ( location ) if location_data is not None : latitude = location_data . latitude longitude = location_data . longitude return latitude , longitude else : return None # Read the spreadsheet data into a DataFrame data = pd . read_excel ( \"data.xlsx\" ) # Replace \"data.xlsx\" with the path to your spreadsheet file states_column = data [ \"states\" ] # Iterate over the states column and retrieve coordinates for state in states_column : coordinates = get_coordinates ( state ) if coordinates is not None : latitude , longitude = coordinates print ( f \"Coordinates for { state } : Latitude= { latitude } , Longitude= { longitude } \" ) else : print ( f \"Coordinates not found for { state } \" ) In the code above, the pandas library is used to read the data from the spreadsheet file ( data.xlsx ), assuming it has a column named \"states\". The values from the \"states\" column are then iterated, and the get_coordinates() function is called for each state name to retrieve its latitude and longitude coordinates. Make sure to update the \"data.xlsx\" file path with the actual path to your spreadsheet file. Additionally, ensure that you have the pandas and geopy libraries installed ( pip install pandas geopy ) before running the code.","title":"Chat GPT"},{"location":"chatgpt/#assigning-a-role-setting-the-context-for-success","text":"Assigning a role to ChatGPT sets the stage for focused and accurate responses. Providing context helps ChatGPT align its responses with the intended purpose. Designating ChatGPT as a teacher, interpreter, or researcher clarifies its role and elicits responses aligned with scientific objectives.","title":"Assigning a Role: Setting the Context for Success"},{"location":"chatgpt/#detailed-instructions-navigating-the-path-of-precision","text":"Precision is essential in extracting valuable insights from ChatGPT. Articulating scientific inquiries with specificity ensures clarity. Formulating detailed questions and tasks guides ChatGPT towards the desired outcome.","title":"Detailed Instructions: Navigating the Path of Precision"},{"location":"chatgpt/#specifying-output-format-tailoring-knowledge-delivery","text":"The choice of output format is crucial for effective scientific communication. Specifying how information is presented enhances comprehension and integration. Output formats can include paragraphs, code snippets, visualizations, or poetic expressions. By adhering to these three principles, we unlock the full potential of ChatGPT, ensuring accuracy and precision in our quest for knowledge and harnessing the limitless possibilities of artificial intelligence. Here are some examples showcasing how the interplay of role assignment, specific prompts, and defined output formats can yield fruitful outcomes:","title":"Specifying Output Format: Tailoring Knowledge Delivery"},{"location":"chatgpt/#examples","text":"Prompt: you are a python interpreter and coder. please write me some code that would take state names, city names or county names and translate that into longitude and latitude qoordinates so that they can be used for computing Responce: To convert state, city, or county names into latitude and longitude coordinates, you can use a geocoding service or a geocoding library in Python. One popular geocoding library is geopy , which provides an interface to various geocoding services. Here's an example of how you can use geopy to retrieve the coordinates: from geopy.geocoders import Nominatim def get_coordinates ( location ): geolocator = Nominatim ( user_agent = \"my-application\" ) location_data = geolocator . geocode ( location ) if location_data is not None : latitude = location_data . latitude longitude = location_data . longitude return latitude , longitude else : return None # Example usage location = \"New York, NY\" coordinates = get_coordinates ( location ) if coordinates is not None : latitude , longitude = coordinates print ( f \"Coordinates for { location } : Latitude= { latitude } , Longitude= { longitude } \" ) else : print ( \"Coordinates not found for the location.\" ) In the code above, the get_coordinates() function takes a location name as input and uses the Nominatim geocoder from geopy to retrieve the coordinates. It returns a tuple of latitude and longitude if the coordinates are found, or None if the coordinates are not found. You can call the get_coordinates() function with state names, city names, or county names to retrieve their respective latitude and longitude coordinates. Remember to install the geopy library by running pip install geopy before executing the code. Clarification: can you make it so that it could draw this data from a spreadsheet that has many different names on it, but they are all in a collum called \"states\" Responce: Certainly! To read the data from a spreadsheet column named \"states\" and retrieve the latitude and longitude coordinates for each state name, you can use the pandas library to load the spreadsheet data and iterate over the \"states\" column. Here's an updated version of the code: import pandas as pd from geopy.geocoders import Nominatim def get_coordinates ( location ): geolocator = Nominatim ( user_agent = \"my-application\" ) location_data = geolocator . geocode ( location ) if location_data is not None : latitude = location_data . latitude longitude = location_data . longitude return latitude , longitude else : return None # Read the spreadsheet data into a DataFrame data = pd . read_excel ( \"data.xlsx\" ) # Replace \"data.xlsx\" with the path to your spreadsheet file states_column = data [ \"states\" ] # Iterate over the states column and retrieve coordinates for state in states_column : coordinates = get_coordinates ( state ) if coordinates is not None : latitude , longitude = coordinates print ( f \"Coordinates for { state } : Latitude= { latitude } , Longitude= { longitude } \" ) else : print ( f \"Coordinates not found for { state } \" ) In the code above, the pandas library is used to read the data from the spreadsheet file ( data.xlsx ), assuming it has a column named \"states\". The values from the \"states\" column are then iterated, and the get_coordinates() function is called for each state name to retrieve its latitude and longitude coordinates. Make sure to update the \"data.xlsx\" file path with the actual path to your spreadsheet file. Additionally, ensure that you have the pandas and geopy libraries installed ( pip install pandas geopy ) before running the code.","title":"Examples:"},{"location":"cyverse/","text":"CyVerse \u00b6 Introduction \u00b6 CyVerse serves as a way for life scientists to share their data with others around the world through a common cyberinfrastructure. It is funded by the National Science Foudation's Directorate for Biological Science and is currently led by the University of Arizona. CyVerse's cyberinfrastructure allows scientists to store their data as well as share it with others through cloud computing for further analysis. Acting as a way to complete complex analyses and share large datasets, CyVerse furthers open science and enables a collaborative workspace. VICE Apps \u00b6 The Visual and Interactive Computing Environment(VICE) allows scientists to run interactive applications through CyVerse. Through this scientists can open their interactive applications(Jupyter Lab, RStudio, Shiny, WebGL, HTML5, VNC, and XPRA), transfer data into containters, analyze this data, and send their results to the cloud.","title":"CyVerse"},{"location":"cyverse/#cyverse","text":"","title":"CyVerse"},{"location":"cyverse/#introduction","text":"CyVerse serves as a way for life scientists to share their data with others around the world through a common cyberinfrastructure. It is funded by the National Science Foudation's Directorate for Biological Science and is currently led by the University of Arizona. CyVerse's cyberinfrastructure allows scientists to store their data as well as share it with others through cloud computing for further analysis. Acting as a way to complete complex analyses and share large datasets, CyVerse furthers open science and enables a collaborative workspace.","title":"Introduction"},{"location":"cyverse/#vice-apps","text":"The Visual and Interactive Computing Environment(VICE) allows scientists to run interactive applications through CyVerse. Through this scientists can open their interactive applications(Jupyter Lab, RStudio, Shiny, WebGL, HTML5, VNC, and XPRA), transfer data into containters, analyze this data, and send their results to the cloud.","title":"VICE Apps"},{"location":"github/","text":"Github Actions \u00b6 Introduction \u00b6 GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts. Key Vocabulary \u00b6 Workflows \u00b6 Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event. Events \u00b6 Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests. Runners \u00b6 Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub. Jobs \u00b6 A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order. Steps \u00b6 A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data. Actions \u00b6 Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. Figure credit : GitHub Docs The components of GitHub Actions that work together to run jobs GitHub Education \u00b6 Steps to Enroll \u00b6 Go to the GitHub Education Site and enter your education status as student From here your school email and dated documentation of your enrollment is required After this is approved you have access to the GitHub education student developer pack! What's Included and Functionality \u00b6 As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. note: the student developer pack doesn't include access to GitHub CodeSpaces","title":"GitHub"},{"location":"github/#github-actions","text":"","title":"Github Actions"},{"location":"github/#introduction","text":"GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts.","title":"Introduction"},{"location":"github/#key-vocabulary","text":"","title":"Key Vocabulary"},{"location":"github/#workflows","text":"Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event.","title":"Workflows"},{"location":"github/#events","text":"Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests.","title":"Events"},{"location":"github/#runners","text":"Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub.","title":"Runners"},{"location":"github/#jobs","text":"A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order.","title":"Jobs"},{"location":"github/#steps","text":"A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data.","title":"Steps"},{"location":"github/#actions","text":"Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. Figure credit : GitHub Docs The components of GitHub Actions that work together to run jobs","title":"Actions"},{"location":"github/#github-education","text":"","title":"GitHub Education"},{"location":"github/#steps-to-enroll","text":"Go to the GitHub Education Site and enter your education status as student From here your school email and dated documentation of your enrollment is required After this is approved you have access to the GitHub education student developer pack!","title":"Steps to Enroll"},{"location":"github/#whats-included-and-functionality","text":"As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. note: the student developer pack doesn't include access to GitHub CodeSpaces","title":"What's Included and Functionality"},{"location":"githubactions/","text":"Introduction \u00b6 GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts. Key Vocabulary \u00b6 Workflows \u00b6 Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event. Events \u00b6 Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests. Runners \u00b6 Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub. Jobs \u00b6 A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order. Steps \u00b6 A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data. Actions \u00b6 Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. Figure credit : GitHub Docs The components of GitHub Actions that work together to run jobs","title":"Githubactions"},{"location":"githubactions/#introduction","text":"GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts.","title":"Introduction"},{"location":"githubactions/#key-vocabulary","text":"","title":"Key Vocabulary"},{"location":"githubactions/#workflows","text":"Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event.","title":"Workflows"},{"location":"githubactions/#events","text":"Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests.","title":"Events"},{"location":"githubactions/#runners","text":"Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub.","title":"Runners"},{"location":"githubactions/#jobs","text":"A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order.","title":"Jobs"},{"location":"githubactions/#steps","text":"A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data.","title":"Steps"},{"location":"githubactions/#actions","text":"Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. Figure credit : GitHub Docs The components of GitHub Actions that work together to run jobs","title":"Actions"},{"location":"githubed/","text":"GitHub Education \u00b6 Steps to Enroll \u00b6 Go to the GitHub Education Site and enter your education status as student From here your school email and dated documentation of your enrollment is required After this is approved you have access to the GitHub education student developer pack! What's Included and Functionality \u00b6 As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. note: the student developer pack doesn't include access to GitHub CodeSpaces","title":"GitHub Education"},{"location":"githubed/#github-education","text":"","title":"GitHub Education"},{"location":"githubed/#steps-to-enroll","text":"Go to the GitHub Education Site and enter your education status as student From here your school email and dated documentation of your enrollment is required After this is approved you have access to the GitHub education student developer pack!","title":"Steps to Enroll"},{"location":"githubed/#whats-included-and-functionality","text":"As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. note: the student developer pack doesn't include access to GitHub CodeSpaces","title":"What's Included and Functionality"},{"location":"jupyter/","text":"Jupyter Notebooks \u00b6 What is it? \u00b6 seamless way to write and iterate python code to perform data analysis Notebooks allows code, figures, diagrams, charts, and explanations to all be stored in one location This allows developer's work to be shared, leading to collaboration and improving reproducibility Additionally, Jupyter Notebooks is free of charge, improving equity in data analysis and software development When new notebooks are created prebuilt docker containers are used to put the notebooks at their own path How does it work? \u00b6 The basis for Jupyter Notebooks is IPython, a command line shell for writing code in the terminal Juputer Notebooks allows this code to be written and iterated in the browser through the use of the ipykernel lines of code can be run all at once or one at a time Jupyter Notebooks supports mulitple languages, most common is python Allows storage of code and inclusion of Markdown files for notes and documentation when new notebooks are created prebuilt docker containers are used to place the notebooks on their own paths Structure \u00b6 Kernel \u00b6 the kernel is specific to the programming language, this project will be in python process that supports the notebook to execute the written code Juptyer team maintains the ipykernel, but other user maintained kernels are available for use Cell \u00b6 the cells are the main contents of the notebooks and are where code is written markdown cells can be created to store notes and info on code green = code running grey = code not running","title":"Jupyter Notebooks"},{"location":"jupyter/#jupyter-notebooks","text":"","title":"Jupyter Notebooks"},{"location":"jupyter/#what-is-it","text":"seamless way to write and iterate python code to perform data analysis Notebooks allows code, figures, diagrams, charts, and explanations to all be stored in one location This allows developer's work to be shared, leading to collaboration and improving reproducibility Additionally, Jupyter Notebooks is free of charge, improving equity in data analysis and software development When new notebooks are created prebuilt docker containers are used to put the notebooks at their own path","title":"What is it?"},{"location":"jupyter/#how-does-it-work","text":"The basis for Jupyter Notebooks is IPython, a command line shell for writing code in the terminal Juputer Notebooks allows this code to be written and iterated in the browser through the use of the ipykernel lines of code can be run all at once or one at a time Jupyter Notebooks supports mulitple languages, most common is python Allows storage of code and inclusion of Markdown files for notes and documentation when new notebooks are created prebuilt docker containers are used to place the notebooks on their own paths","title":"How does it work?"},{"location":"jupyter/#structure","text":"","title":"Structure"},{"location":"jupyter/#kernel","text":"the kernel is specific to the programming language, this project will be in python process that supports the notebook to execute the written code Juptyer team maintains the ipykernel, but other user maintained kernels are available for use","title":"Kernel"},{"location":"jupyter/#cell","text":"the cells are the main contents of the notebooks and are where code is written markdown cells can be created to store notes and info on code green = code running grey = code not running","title":"Cell"},{"location":"jupyternotebooks/","text":"","title":"Jupyternotebooks"},{"location":"keysassignments/","text":"Keys Assignments \u00b6 Assignment 1: Internship Description \u00b6 Project Description \u00b6 This summer, I have the privilege of working in Dr. Swetnam's lab, delving into the fascinating intersection of AI and scientific research. Dr. Swetnam, a Research Associate Professor of Geoinformatics and Joint faculty member in the School of Natural Resources and Environment, is actively involved in the CyVerse initiative, which promotes open science. Among his various projects, he is currently exploring the applications of AI in scientific fields. My specific focus this summer revolves around the utilization of AI tools such as Chat GPT and Google Bard to examine their impact on data analysis in the context of epidemic-related data (e.g., Covid-19, Cholera). The goal is to develop a reusable Python code that can effectively generate heat maps for hotspot mapping. By addressing the transformative influence of AI on scientific endeavors, we aim to foster a more accessible and collaborative scientific community. In essence, this research journey embraces the promising prospects of AI, forging new paths in scientific inquiry while cultivating a more inclusive approach to knowledge discovery. Assignment 2: Introduction to your Research \u00b6 Purpose: \u00b6 As artificial intelligence (AI) continues to exert a growing influence on our society and scientific endeavors, it becomes imperative that we adapt to its presence. The way we interact with data must evolve, and it is crucial to harness the potential of these new tools effectively. This project focuses on utilizing AI tools to analyze epidemic-related data, specifically data on COVID-19 and Cholera, and exploring their efficiency in understanding and mitigating these diseases. The research undertaken aims to facilitate the widespread integration of AI in the field of data science, accompanied by the development of a reusable program. One of the key objectives is to create a Python code capable of evaluating specialized data and generating heat maps, which are graphical representations that visually highlight areas of concentration or origin for epidemic outbreaks. Previous Research: \u00b6 As AI continues to gain influence, particularly in the field of data science, it offers numerous implications. These include accelerating the discovery of relationships within complex data sets and automating repetitive tasks, leading to more efficient and accurate data analysis (Brookings, 2023; Our World in Data, 2022). Heatmaps are visual representations that utilize colors to display the intensity or density of certain data points on a geographical or grid-based layout (NIEHS, n.d.). In the context of epidemic-related data analysis, heatmaps can be used to identify areas of concentration or origin for diseases. For example, Michael Worobey and his team utilized heatmaps to analyze the genetic diversity of COVID-19 and identified the Huanan Seafood Market in Wuhan as a potential origin (Zimmer et al., 2020). This approach is reminiscent of the work of John Snow, a pioneer in epidemiology, who famously used a heatmap to track the cases of cholera during the 1854 outbreak in London, ultimately identifying contaminated water as the source (BBC, n.d.). Need For Study: \u00b6 By harnessing the promising prospects of AI, this research journey not only pioneers novel paths in scientific inquiry but also fosters a more inclusive approach to knowledge discovery. Through the development of this Python code, the project strives to implement the regular use of AI in the data science field while creating a reusable program that is open for anyone to use. In essence, it embraces the potential of AI, forging new paths in scientific inquiry and data analysis, all while cultivating a more inclusive and effective approach to understanding and combating epidemics like COVID-19 and cholera. Problem Statement: \u00b6 Research Question: How can AI tools be effectively utilized to analyze epidemic-related data and improve our understanding and response to diseases like COVID-19 and cholera? Hypotheisis: The utilization of AI tools, specifically Chat GPT and Google Bard, will enhance our understanding and mitigation of diseases like COVID-19 and cholera, facilitating the integration of AI in data science. References \u00b6 References: - Brookings. (2023). How artificial intelligence is transforming the world. Retrieved from https://www.brookings.edu/research/how-artificial-intelligence-is-transforming-the-world/ - Our World in Data. (2022). The brief history of artificial intelligence: The world has changed. Retrieved from https://ourworldindata.org/brief-history-of-ai - National Institute of Environmental Health Sciences (NIEHS). (n.d.). Data visualization: Heatmaps. Retrieved from https://tools.niehs.nih.gov/cehsweb/assets/fileassets/datavisualization/heatmap.pdf - Worobey, M. (2021). The Origins of COVID-19. Scientific American, 325(1), 46-53. - now, J. (1855). On the Mode of Communication of Cholera (2 nd ed.). John Churchill. Assignment 3: Materials and Methods \u00b6 Assignment 4: Results \u00b6 Assignment 5 Long Abstract \u00b6 Assignment 6 Conclusion and Discussion \u00b6 Assignment 7 Title and Short Abstract \u00b6 Title \u00b6 Short Abstract \u00b6","title":"Assignments"},{"location":"keysassignments/#keys-assignments","text":"","title":"Keys Assignments"},{"location":"keysassignments/#assignment-1-internship-description","text":"","title":"Assignment 1: Internship Description"},{"location":"keysassignments/#project-description","text":"This summer, I have the privilege of working in Dr. Swetnam's lab, delving into the fascinating intersection of AI and scientific research. Dr. Swetnam, a Research Associate Professor of Geoinformatics and Joint faculty member in the School of Natural Resources and Environment, is actively involved in the CyVerse initiative, which promotes open science. Among his various projects, he is currently exploring the applications of AI in scientific fields. My specific focus this summer revolves around the utilization of AI tools such as Chat GPT and Google Bard to examine their impact on data analysis in the context of epidemic-related data (e.g., Covid-19, Cholera). The goal is to develop a reusable Python code that can effectively generate heat maps for hotspot mapping. By addressing the transformative influence of AI on scientific endeavors, we aim to foster a more accessible and collaborative scientific community. In essence, this research journey embraces the promising prospects of AI, forging new paths in scientific inquiry while cultivating a more inclusive approach to knowledge discovery.","title":"Project Description"},{"location":"keysassignments/#assignment-2-introduction-to-your-research","text":"","title":"Assignment 2: Introduction to your Research"},{"location":"keysassignments/#purpose","text":"As artificial intelligence (AI) continues to exert a growing influence on our society and scientific endeavors, it becomes imperative that we adapt to its presence. The way we interact with data must evolve, and it is crucial to harness the potential of these new tools effectively. This project focuses on utilizing AI tools to analyze epidemic-related data, specifically data on COVID-19 and Cholera, and exploring their efficiency in understanding and mitigating these diseases. The research undertaken aims to facilitate the widespread integration of AI in the field of data science, accompanied by the development of a reusable program. One of the key objectives is to create a Python code capable of evaluating specialized data and generating heat maps, which are graphical representations that visually highlight areas of concentration or origin for epidemic outbreaks.","title":"Purpose:"},{"location":"keysassignments/#previous-research","text":"As AI continues to gain influence, particularly in the field of data science, it offers numerous implications. These include accelerating the discovery of relationships within complex data sets and automating repetitive tasks, leading to more efficient and accurate data analysis (Brookings, 2023; Our World in Data, 2022). Heatmaps are visual representations that utilize colors to display the intensity or density of certain data points on a geographical or grid-based layout (NIEHS, n.d.). In the context of epidemic-related data analysis, heatmaps can be used to identify areas of concentration or origin for diseases. For example, Michael Worobey and his team utilized heatmaps to analyze the genetic diversity of COVID-19 and identified the Huanan Seafood Market in Wuhan as a potential origin (Zimmer et al., 2020). This approach is reminiscent of the work of John Snow, a pioneer in epidemiology, who famously used a heatmap to track the cases of cholera during the 1854 outbreak in London, ultimately identifying contaminated water as the source (BBC, n.d.).","title":"Previous Research:"},{"location":"keysassignments/#need-for-study","text":"By harnessing the promising prospects of AI, this research journey not only pioneers novel paths in scientific inquiry but also fosters a more inclusive approach to knowledge discovery. Through the development of this Python code, the project strives to implement the regular use of AI in the data science field while creating a reusable program that is open for anyone to use. In essence, it embraces the potential of AI, forging new paths in scientific inquiry and data analysis, all while cultivating a more inclusive and effective approach to understanding and combating epidemics like COVID-19 and cholera.","title":"Need For Study:"},{"location":"keysassignments/#problem-statement","text":"Research Question: How can AI tools be effectively utilized to analyze epidemic-related data and improve our understanding and response to diseases like COVID-19 and cholera? Hypotheisis: The utilization of AI tools, specifically Chat GPT and Google Bard, will enhance our understanding and mitigation of diseases like COVID-19 and cholera, facilitating the integration of AI in data science.","title":"Problem Statement:"},{"location":"keysassignments/#references","text":"References: - Brookings. (2023). How artificial intelligence is transforming the world. Retrieved from https://www.brookings.edu/research/how-artificial-intelligence-is-transforming-the-world/ - Our World in Data. (2022). The brief history of artificial intelligence: The world has changed. Retrieved from https://ourworldindata.org/brief-history-of-ai - National Institute of Environmental Health Sciences (NIEHS). (n.d.). Data visualization: Heatmaps. Retrieved from https://tools.niehs.nih.gov/cehsweb/assets/fileassets/datavisualization/heatmap.pdf - Worobey, M. (2021). The Origins of COVID-19. Scientific American, 325(1), 46-53. - now, J. (1855). On the Mode of Communication of Cholera (2 nd ed.). John Churchill.","title":"References"},{"location":"keysassignments/#assignment-3-materials-and-methods","text":"","title":"Assignment 3: Materials and Methods"},{"location":"keysassignments/#assignment-4-results","text":"","title":"Assignment 4: Results"},{"location":"keysassignments/#assignment-5-long-abstract","text":"","title":"Assignment 5 Long Abstract"},{"location":"keysassignments/#assignment-6-conclusion-and-discussion","text":"","title":"Assignment 6 Conclusion and Discussion"},{"location":"keysassignments/#assignment-7-title-and-short-abstract","text":"","title":"Assignment 7 Title and Short Abstract"},{"location":"keysassignments/#title","text":"","title":"Title"},{"location":"keysassignments/#short-abstract","text":"","title":"Short Abstract"},{"location":"logbook/","text":"Logbook \u00b6 (6/5 - 6/9) Training Week: This past week I went through Training Week with KEYS. Although it was a lot of information for one week I learned a lot about many things, from science literacy to Git Bash coding. I also got to work with Dr. Swetnam on the groundwork for my github website so that on monday I can start working on my project asap. We also started talking about project ideas, and I am looking into the idea of doing a project related to COVID-19 tracking. (6/12): Today I worked on solidifying my project. I found datasets from the first breakout of COVID-19 and from the Cholera outbreak in London that was investegated by Jhon Snow. It took me a while, but once I found them I started looking into what it would take to create a heat map of the data. I struggled a bit with how to get the data into a platform where I could start to mess with it, but I am going to work on that more tomorrow. I also completed the KEYS assignments for this week that I could. (6/13): Today I looked at enviornments and how to use python in codespaces. I had an issue for most of the morning because I was working on the wrong repository (before I made a new one) and it was confusing as to where I did and did not have things installed. But I worked with Dr.Swetnam this afternoon to set up a new repository that will work better. Additionally, I have been having troubble changing my css file for this website, but I finally figured it out today and changed some things. I also looked into other heat maps, and it seems like there is a lot of options out there. Tomorrow I am planning on working with some of those different heat maps in jypter notebooks along with looking at how those are different/similar to what I want to make. (6/14): Today I went to my first science seminar with KEYS. After that I worked on KEYS assignments and wrote my Introduction. In the afternoon I worked on codespaces and tried to get my enviornment to a place where it could run a heat map, but there is something wrong with the python that I am going to have to troubbleshoot. I got help from Dr. Swetnam and we got Chat GPT to write a code for a heat map. We also created a set of fake data to test it, and when running jupyter labs on his computer it worked. Tomorrow I need to work on getting my datasets into the program and fixing my conda enviornment so it can actually run it on my computer. (6/15): Today I started by working on my enviornment. After running the right commands it would not open the right enviornment and it kept telling me to run 'git init'. It took a really long time for me to figure out what was wrong, but in the end I had to use the 'git init bash' command and then I was able to open my enviornment. After that I worked on getting my data uploaded and I was actually able to create a heatmap for the COVID-19 data I had gathered that was accurate. Unfortunatly I was unable to get my Cholera data to generate right, because the code itself didn't have the information I needed (it was in meeters not in longitude and lattitude). Even so, I ended up annotating my jupyter notebook and I am at a good spot. Next week I am going to work on either obtaining new data or attempting to change the Cholera data along with making my jupyter notebook visable on my website. (6/20): Today I worked on making mu jupyter notebook viewable on my website. With the help of Chat GPY I was able to figure out how to make the notebook into HTML so that you could see it on the website. The only issue I ran into was that I need to make a website for the repository so that I dont need to copy every notebook over. Other than that I worked a little on data collection for a new set of data but I had a hard time finding a dataset anf I realised i need a very specific instance of illness because otherwise there is no accurate datasets. I am going to work on that and the jupyter notebooks tomorrow. (6/21): Today I went to our second Science Seminar with KEYS. When I was done with that I worked on having my jupyter notebook be acessable. At one point I had it working so it would open the html file on its own, but would remove you from the website. I wanted to make it so that I could embed it in the website however, so I worked on that. I was able to make it embed in the website, but unfortunatley there was a 404 error that I am now having troubble fixing. I am not entierly sure what is wrong, but I hope to work on it more tomorrow. I also found a great collection of data on data.gov, but it is going to take me a bit to sort through it and find what I need. (6/22): Today I continued working on making my jupyter notebook acessable on my website. I also found some good datasets on data.gov, I just need to write a program that will change states, cities, and counties into loongitude and lattitude coorsinates. With my jupyter notebook I got it to where when you clicked on one of the things in the nav tree it would download the notebook file, but that is not how I wanted it to work. I spent the morning and some of the afternoon getting it to the point that in codespaces when I used mkdocs serve it would show the right view of my jupyter notebook, but when I went to push the changes I made to my github website it failed. It kept saying 'error mkdocs-jupyter' was not installed, even though I put it in the requirements.txt file and it said it was downloaded when we ran pip freeze to check the installed packages. I eneded up sitting with Michele for 2 hours trying to make it work, but we could not figure it out. That is the first thing I am going to work on next week.","title":"Logbook"},{"location":"logbook/#logbook","text":"(6/5 - 6/9) Training Week: This past week I went through Training Week with KEYS. Although it was a lot of information for one week I learned a lot about many things, from science literacy to Git Bash coding. I also got to work with Dr. Swetnam on the groundwork for my github website so that on monday I can start working on my project asap. We also started talking about project ideas, and I am looking into the idea of doing a project related to COVID-19 tracking. (6/12): Today I worked on solidifying my project. I found datasets from the first breakout of COVID-19 and from the Cholera outbreak in London that was investegated by Jhon Snow. It took me a while, but once I found them I started looking into what it would take to create a heat map of the data. I struggled a bit with how to get the data into a platform where I could start to mess with it, but I am going to work on that more tomorrow. I also completed the KEYS assignments for this week that I could. (6/13): Today I looked at enviornments and how to use python in codespaces. I had an issue for most of the morning because I was working on the wrong repository (before I made a new one) and it was confusing as to where I did and did not have things installed. But I worked with Dr.Swetnam this afternoon to set up a new repository that will work better. Additionally, I have been having troubble changing my css file for this website, but I finally figured it out today and changed some things. I also looked into other heat maps, and it seems like there is a lot of options out there. Tomorrow I am planning on working with some of those different heat maps in jypter notebooks along with looking at how those are different/similar to what I want to make. (6/14): Today I went to my first science seminar with KEYS. After that I worked on KEYS assignments and wrote my Introduction. In the afternoon I worked on codespaces and tried to get my enviornment to a place where it could run a heat map, but there is something wrong with the python that I am going to have to troubbleshoot. I got help from Dr. Swetnam and we got Chat GPT to write a code for a heat map. We also created a set of fake data to test it, and when running jupyter labs on his computer it worked. Tomorrow I need to work on getting my datasets into the program and fixing my conda enviornment so it can actually run it on my computer. (6/15): Today I started by working on my enviornment. After running the right commands it would not open the right enviornment and it kept telling me to run 'git init'. It took a really long time for me to figure out what was wrong, but in the end I had to use the 'git init bash' command and then I was able to open my enviornment. After that I worked on getting my data uploaded and I was actually able to create a heatmap for the COVID-19 data I had gathered that was accurate. Unfortunatly I was unable to get my Cholera data to generate right, because the code itself didn't have the information I needed (it was in meeters not in longitude and lattitude). Even so, I ended up annotating my jupyter notebook and I am at a good spot. Next week I am going to work on either obtaining new data or attempting to change the Cholera data along with making my jupyter notebook visable on my website. (6/20): Today I worked on making mu jupyter notebook viewable on my website. With the help of Chat GPY I was able to figure out how to make the notebook into HTML so that you could see it on the website. The only issue I ran into was that I need to make a website for the repository so that I dont need to copy every notebook over. Other than that I worked a little on data collection for a new set of data but I had a hard time finding a dataset anf I realised i need a very specific instance of illness because otherwise there is no accurate datasets. I am going to work on that and the jupyter notebooks tomorrow. (6/21): Today I went to our second Science Seminar with KEYS. When I was done with that I worked on having my jupyter notebook be acessable. At one point I had it working so it would open the html file on its own, but would remove you from the website. I wanted to make it so that I could embed it in the website however, so I worked on that. I was able to make it embed in the website, but unfortunatley there was a 404 error that I am now having troubble fixing. I am not entierly sure what is wrong, but I hope to work on it more tomorrow. I also found a great collection of data on data.gov, but it is going to take me a bit to sort through it and find what I need. (6/22): Today I continued working on making my jupyter notebook acessable on my website. I also found some good datasets on data.gov, I just need to write a program that will change states, cities, and counties into loongitude and lattitude coorsinates. With my jupyter notebook I got it to where when you clicked on one of the things in the nav tree it would download the notebook file, but that is not how I wanted it to work. I spent the morning and some of the afternoon getting it to the point that in codespaces when I used mkdocs serve it would show the right view of my jupyter notebook, but when I went to push the changes I made to my github website it failed. It kept saying 'error mkdocs-jupyter' was not installed, even though I put it in the requirements.txt file and it said it was downloaded when we ran pip freeze to check the installed packages. I eneded up sitting with Michele for 2 hours trying to make it work, but we could not figure it out. That is the first thing I am going to work on next week.","title":"Logbook"},{"location":"myproject/","text":"My Project \u00b6 This summer my project focused on the use of AI like Chat GPT in a data science field. I focused on being able to use Chat GPT to evaluate epedemic related datasets such as the first cases of COVID-19 and ____. By using Chat GPT I was able to create a python code that can evaluate any set of data that includes a longitude and lattitude and create a heatmap from it. The aim of this project was to implement the common and ethical use of AI such as Chat GPT in data science while also creating more open and acessable science such as heatmaps.","title":"My Project"},{"location":"myproject/#my-project","text":"This summer my project focused on the use of AI like Chat GPT in a data science field. I focused on being able to use Chat GPT to evaluate epedemic related datasets such as the first cases of COVID-19 and ____. By using Chat GPT I was able to create a python code that can evaluate any set of data that includes a longitude and lattitude and create a heatmap from it. The aim of this project was to implement the common and ethical use of AI such as Chat GPT in data science while also creating more open and acessable science such as heatmaps.","title":"My Project"},{"location":"poster/","text":"","title":"Poster"},{"location":"python/","text":"Python Basics: \u00b6 Python Functions: Reusable blocks of code that perform specific tasks, enabling modular programming. They encapsulate functionality and promote code reuse. Python Classes: Blueprints for creating objects and facilitating object-oriented programming. They define properties (attributes) and behaviors (methods) of objects. Python Packages: Collections of modules and resources that enhance functionality and simplify application development. They provide pre-built code and tools for specific Data Science: \u00b6 Data Cleaning: Process of identifying and correcting or removing errors, inconsistencies, and inaccuracies in datasets to ensure data quality. Data Analysis: Techniques for exploring, transforming, and deriving insights from data to support decision-making and problem-solving. Machine Learning: Algorithms and models that enable systems to learn from data and make predictions or decisions without explicit programming. Web Development: \u00b6 HTML: Markup language used for structuring the content and layout of web pages. CSS: Stylesheet language used for describing the presentation and visual styling of web pages. JavaScript: Programming language that adds interactivity and dynamic behavior to websites. Artificial Intelligence (AI): \u00b6 Machine Learning: Subset of AI that focuses on developing algorithms and models that enable systems to learn from data and make predictions or decisions without explicit programming. Natural Language Processing (NLP): AI technique that enables computers to understand, interpret, and generate human language. Computer Vision: AI field that deals with enabling computers to see, interpret, and understand visual information.","title":"Coding, Data Science & AI"},{"location":"python/#python-basics","text":"Python Functions: Reusable blocks of code that perform specific tasks, enabling modular programming. They encapsulate functionality and promote code reuse. Python Classes: Blueprints for creating objects and facilitating object-oriented programming. They define properties (attributes) and behaviors (methods) of objects. Python Packages: Collections of modules and resources that enhance functionality and simplify application development. They provide pre-built code and tools for specific","title":"Python Basics:"},{"location":"python/#data-science","text":"Data Cleaning: Process of identifying and correcting or removing errors, inconsistencies, and inaccuracies in datasets to ensure data quality. Data Analysis: Techniques for exploring, transforming, and deriving insights from data to support decision-making and problem-solving. Machine Learning: Algorithms and models that enable systems to learn from data and make predictions or decisions without explicit programming.","title":"Data Science:"},{"location":"python/#web-development","text":"HTML: Markup language used for structuring the content and layout of web pages. CSS: Stylesheet language used for describing the presentation and visual styling of web pages. JavaScript: Programming language that adds interactivity and dynamic behavior to websites.","title":"Web Development:"},{"location":"python/#artificial-intelligence-ai","text":"Machine Learning: Subset of AI that focuses on developing algorithms and models that enable systems to learn from data and make predictions or decisions without explicit programming. Natural Language Processing (NLP): AI technique that enables computers to understand, interpret, and generate human language. Computer Vision: AI field that deals with enabling computers to see, interpret, and understand visual information.","title":"Artificial Intelligence (AI):"},{"location":"references/","text":"References \u00b6","title":"References"},{"location":"references/#references","text":"","title":"References"},{"location":"assets/readme/","text":"images go in this folder","title":"Readme"},{"location":"unused/chatgptbard/","text":"","title":"Chatgptbard"},{"location":"unused/chatgptcopilot/","text":"","title":"Chatgptcopilot"},{"location":"unused/githubcodespaces/","text":"this is some super cool information \u00b6","title":"Githubcodespaces"},{"location":"unused/githubcodespaces/#this-is-some-super-cool-information","text":"","title":"this is some super cool information"}]}